#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘ç»œå®‰å…¨æ–°é—»è‡ªåŠ¨æŠ“å–ç¨‹åº
è‡ªåŠ¨æŠ“å–å½“å¤©æœ€æ–°çš„ç½‘ç»œå®‰å…¨æ–°é—»å¹¶ç”ŸæˆHTMLæ–‡ä»¶
"""

import requests
import feedparser
from datetime import datetime, date
import re
import os
from bs4 import BeautifulSoup
import time
import random
from urllib.parse import urljoin, urlparse
import logging

# å¯¼å…¥é…ç½®æ–‡ä»¶
try:
    from scraper_config import NEWS_SOURCES, SECURITY_KEYWORDS, USER_AGENTS, REQUEST_CONFIG, FILE_CONFIG, LOG_CONFIG
except ImportError:
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    NEWS_SOURCES = [
        {
            'name': 'å®‰å…¨å®¢',
            'rss_url': 'https://api.anquanke.com/data/v1/rss',
            'enabled': True,
            'weight': 1.0
        },
        {
            'name': 'FreeBuf',
            'rss_url': 'https://www.freebuf.com/feed',
            'enabled': True,
            'weight': 1.0
        },
        {
            'name': 'å˜¶å¼',
            'rss_url': 'https://www.4hou.com/feed',
            'enabled': True,
            'weight': 1.0
        }
    ]
    SECURITY_KEYWORDS = [
        'å®‰å…¨', 'æ¼æ´', 'æ”»å‡»', 'é»‘å®¢', 'ç—…æ¯’', 'æ¶æ„è½¯ä»¶', 'å‹’ç´¢', 'æ¸—é€',
        'é˜²æŠ¤', 'é˜²å¾¡', 'åŠ å¯†', 'è§£å¯†', 'éšç§', 'æ•°æ®æ³„éœ²', 'ç½‘ç»œå®‰å…¨',
        'ä¿¡æ¯å®‰å…¨', 'APT', 'DDoS', 'é’“é±¼', 'æœ¨é©¬', 'åé—¨', 'ææƒ',
        'å®‰å…¨æ¼æ´', 'å®‰å…¨äº‹ä»¶', 'å®‰å…¨å¨èƒ', 'å®‰å…¨é˜²æŠ¤', 'å®‰å…¨æ£€æµ‹',
        'ç½‘ç»œå®‰å…¨', 'ç½‘ç»œæ”»å‡»', 'ç½‘ç»œé˜²æŠ¤', 'ç½‘ç»œå¨èƒ'
    ]
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    ]
    REQUEST_CONFIG = {
        'timeout': 10,
        'delay_range': (1, 3),
        'max_retries': 3,
        'retry_delay': 2
    }
    FILE_CONFIG = {
        'output_dir': '.',
        'filename_pattern': 'news{date}.html',
        'date_format': '%Y%m%d',
        'encoding': 'utf-8'
    }
    LOG_CONFIG = {
        'level': 'INFO',
        'format': '%(asctime)s - %(levelname)s - %(message)s',
        'file': 'news_scraper.log'
    }

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, LOG_CONFIG['level']),
    format=LOG_CONFIG['format'],
    handlers=[
        logging.FileHandler(LOG_CONFIG['file'], encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SecurityNewsScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': random.choice(USER_AGENTS)
        })
        self.news_sources = NEWS_SOURCES
        self.today = date.today()
        self.scraped_news = []
        
    def fetch_rss_feed(self, rss_url, source_name):
        """è·å–RSSè®¢é˜…æºçš„æ–°é—»"""
        try:
            logger.info(f"æ­£åœ¨æŠ“å– {source_name} çš„RSSæº: {rss_url}")
            feed = feedparser.parse(rss_url)
            
            today_news = []
            for entry in feed.entries:
                # è§£æå‘å¸ƒæ—¶é—´
                pub_date = None
                if hasattr(entry, 'published_parsed'):
                    pub_date = datetime(*entry.published_parsed[:6]).date()
                elif hasattr(entry, 'updated_parsed'):
                    pub_date = datetime(*entry.updated_parsed[:6]).date()
                
                # åªå¤„ç†ä»Šå¤©çš„æ–°é—»
                if pub_date and pub_date == self.today:
                    news_item = {
                        'title': entry.title,
                        'link': entry.link,
                        'summary': getattr(entry, 'summary', ''),
                        'published_date': pub_date,
                        'source': source_name,
                        'content': ''
                    }
                    
                    # å¦‚æœæœ‰è¯¦ç»†å†…å®¹ï¼Œå°è¯•è·å–
                    if hasattr(entry, 'content'):
                        news_item['content'] = entry.content[0].value if entry.content else ''
                    
                    today_news.append(news_item)
            
            logger.info(f"ä» {source_name} è·å–åˆ° {len(today_news)} æ¡ä»Šæ—¥æ–°é—»")
            return today_news
            
        except Exception as e:
            logger.error(f"æŠ“å– {source_name} RSSæºå¤±è´¥: {e}")
            return []
    
    def clean_html_content(self, html_content):
        """æ¸…ç†HTMLå†…å®¹ï¼Œæå–çº¯æ–‡æœ¬"""
        if not html_content:
            return ""
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()
            
            # è·å–æ–‡æœ¬å†…å®¹
            text = soup.get_text()
            # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            return text[:500] + "..." if len(text) > 500 else text
        except Exception as e:
            logger.error(f"æ¸…ç†HTMLå†…å®¹å¤±è´¥: {e}")
            return html_content[:300] + "..." if len(html_content) > 300 else html_content
    
    def scrape_news_content(self, news_item):
        """æŠ“å–æ–°é—»è¯¦ç»†å†…å®¹"""
        try:
            # æ·»åŠ éšæœºå»¶æ—¶é¿å…è¢«åçˆ¬è™«
            time.sleep(random.uniform(1, 3))
            
            response = self.session.get(news_item['link'], timeout=10)
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°è¯•æå–æ–‡ç« æ­£æ–‡ï¼ˆä¸åŒç½‘ç«™å¯èƒ½æœ‰ä¸åŒçš„ç»“æ„ï¼‰
            content_selectors = [
                'article', '.article-content', '.content', '.post-content',
                '.entry-content', '.post-body', '.article-body'
            ]
            
            content = ""
            for selector in content_selectors:
                content_element = soup.select_one(selector)
                if content_element:
                    content = self.clean_html_content(str(content_element))
                    if len(content) > 100:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                        break
            
            if not content:
                # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šå†…å®¹åŒºåŸŸï¼Œä½¿ç”¨æ•´ä¸ªbody
                body = soup.find('body')
                if body:
                    content = self.clean_html_content(str(body))
            
            return content
        except Exception as e:
            logger.error(f"æŠ“å–æ–°é—»å†…å®¹å¤±è´¥ {news_item['link']}: {e}")
            return ""
    
    def filter_security_news(self, news_list):
        """è¿‡æ»¤ç½‘ç»œå®‰å…¨ç›¸å…³æ–°é—»"""
        filtered_news = []
        for news in news_list:
            title = news['title'].lower()
            summary = news['summary'].lower()
            
            # æ£€æŸ¥æ ‡é¢˜æˆ–æ‘˜è¦æ˜¯å¦åŒ…å«å®‰å…¨ç›¸å…³å…³é”®è¯
            is_security_related = any(keyword in title or keyword in summary 
                                    for keyword in SECURITY_KEYWORDS)
            
            if is_security_related:
                filtered_news.append(news)
        
        return filtered_news
    
    def scrape_all_sources(self):
        """ä»æ‰€æœ‰æºæŠ“å–æ–°é—»"""
        all_news = []
        
        for source in self.news_sources:
            if not source['enabled']:
                continue
                
            try:
                news_list = self.fetch_rss_feed(source['rss_url'], source['name'])
                if news_list:
                    # è¿‡æ»¤ç½‘ç»œå®‰å…¨ç›¸å…³æ–°é—»
                    security_news = self.filter_security_news(news_list)
                    all_news.extend(security_news)
                    logger.info(f"ä» {source['name']} ç­›é€‰å‡º {len(security_news)} æ¡å®‰å…¨ç›¸å…³æ–°é—»")
            except Exception as e:
                logger.error(f"å¤„ç† {source['name']} æ—¶å‡ºé”™: {e}")
        
        # å»é‡ï¼ˆåŸºäºæ ‡é¢˜ï¼‰
        seen_titles = set()
        unique_news = []
        for news in all_news:
            title_key = news['title'].strip().lower()
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        self.scraped_news = unique_news
        logger.info(f"æ€»å…±è·å–åˆ° {len(unique_news)} æ¡ä¸é‡å¤çš„å®‰å…¨æ–°é—»")
        return unique_news
    
    def generate_html_content(self, news_list):
        """ç”Ÿæˆç¬¦åˆç°æœ‰æ ¼å¼çš„HTMLå†…å®¹"""
        if not news_list:
            logger.warning("æ²¡æœ‰æ–°é—»å†…å®¹å¯ç”Ÿæˆ")
            return ""
        
        # æŒ‰æ¥æºåˆ†ç»„
        news_by_source = {}
        for news in news_list:
            source = news['source']
            if source not in news_by_source:
                news_by_source[source] = []
            news_by_source[source].append(news)
        
        # ç”Ÿæˆæ‘˜è¦å†…å®¹
        summary_content = "ä»Šæ—¥ç½‘ç»œå®‰å…¨é¢†åŸŸé‡ç‚¹å…³æ³¨ï¼š"
        for i, news in enumerate(news_list[:3], 1):
            summary_content += f"ã€{i}ã€‘{news['title'][:30]}{'...' if len(news['title']) > 30 else ''}ï¼›"
        summary_content = summary_content.rstrip('ï¼›')
        
        current_time = datetime.now().strftime('%Y/%m/%d %H:%M:%S')
        today_str = self.today.strftime('%Y-%m-%d')
        news_count = len(news_list)
        
        html_content = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>æµ·ä¹‹å®‰å®‰å…¨æ¯æ—¥å¿«æŠ¥ - {today_str} - ç¬¬{news_count}æœŸ</title>
  
    <style>
      * {{
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }}
      
      body {{
        font-family: 'Microsoft YaHei', 'SimHei', 'Arial', sans-serif;
        background: linear-gradient(135deg, #0f172a 0%, #1e293b 50%, #334155 100%);
        color: #f1f5f9;
        min-height: 100vh;
        padding: 20px;
        line-height: 1.6;
      }}
      
      .container {{
        max-width: 1200px;
        margin: 0 auto;
        background: rgba(15, 23, 42, 0.95);
        border: 1px solid rgba(59, 130, 246, 0.3);
        border-radius: 16px;
        box-shadow: 0 25px 50px -12px rgba(0, 0, 0, 0.8), 
                    0 0 0 1px rgba(59, 130, 246, 0.1),
                    inset 0 1px 0 rgba(255, 255, 255, 0.1);
        overflow: hidden;
      }}
      
      .header {{
        background: linear-gradient(90deg, rgba(59, 130, 246, 0.1) 0%, rgba(147, 51, 234, 0.1) 100%);
        border-bottom: 1px solid rgba(59, 130, 246, 0.3);
        padding: 40px;
        text-align: center;
        position: relative;
      }}
      
      .header::before {{
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        height: 2px;
        background: linear-gradient(90deg, #3b82f6, #8b5cf6, #06b6d4);
      }}
      
      .logo {{
        width: 120px;
        height: auto;
        margin-bottom: 20px;
        filter: drop-shadow(0 4px 8px rgba(59, 130, 246, 0.3));
      }}
      
      .title {{
        font-size: 36px;
        font-weight: 700;
        background: linear-gradient(135deg, #3b82f6, #8b5cf6);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        margin-bottom: 16px;
        text-shadow: 0 0 30px rgba(59, 130, 246, 0.5);
      }}
      
      .subtitle {{
        font-size: 18px;
        color: #94a3b8;
        margin-bottom: 8px;
      }}
      
      .content {{
        padding: 40px;
      }}
      
      .summary-section {{
        background: rgba(30, 41, 59, 0.8);
        border: 1px solid rgba(59, 130, 246, 0.2);
        border-radius: 12px;
        padding: 24px;
        margin-bottom: 32px;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.3);
      }}
      
      .summary-title {{
        font-size: 20px;
        font-weight: 600;
        color: #3b82f6;
        margin-bottom: 16px;
        display: flex;
        align-items: center;
        gap: 8px;
      }}
      
      .summary-title::before {{
        content: 'ğŸ“Š';
        font-size: 24px;
      }}
      
      .summary-content {{
        color: #cbd5e1;
        line-height: 1.8;
        font-size: 16px;
      }}
      
      .category-section {{
        margin-bottom: 40px;
        background: rgba(30, 41, 59, 0.4);
        border-radius: 12px;
        border: 1px solid rgba(59, 130, 246, 0.15);
        overflow: hidden;
      }}
      
      .category-header {{
        background: linear-gradient(135deg, rgba(59, 130, 246, 0.2), rgba(147, 51, 234, 0.2));
        padding: 20px 24px;
        border-bottom: 1px solid rgba(59, 130, 246, 0.3);
      }}
      
      .category-title {{
        font-size: 24px;
        font-weight: 600;
        color: #e2e8f0;
        display: flex;
        align-items: center;
        gap: 12px;
      }}
      
      .category-news {{
        padding: 24px;
      }}
      
      .news-item {{
        background: rgba(51, 65, 85, 0.6);
        border: 1px solid rgba(59, 130, 246, 0.2);
        border-radius: 8px;
        padding: 20px;
        margin-bottom: 20px;
        transition: all 0.3s ease;
        position: relative;
      }}
      
      .news-item::before {{
        content: '';
        position: absolute;
        left: 0;
        top: 0;
        bottom: 0;
        width: 4px;
        background: linear-gradient(180deg, #3b82f6, #8b5cf6);
        border-radius: 2px 0 0 2px;
      }}
      
      .news-item:hover {{
        border-color: rgba(59, 130, 246, 0.4);
        box-shadow: 0 4px 12px rgba(59, 130, 246, 0.15);
      }}
      
      .news-title {{
        font-size: 18px;
        font-weight: 600;
        color: #f1f5f9;
        margin-bottom: 12px;
        line-height: 1.4;
      }}
      
      .news-analysis {{
        color: #cbd5e1;
        line-height: 1.7;
        font-size: 15px;
      }}
      
      .footer {{
        background: rgba(15, 23, 42, 0.8);
        border-top: 1px solid rgba(59, 130, 246, 0.3);
        padding: 24px 40px;
        text-align: center;
        color: #64748b;
        font-size: 14px;
      }}
      
      /* å›¾æ ‡æ ·å¼ */
      .icon-focus::before {{ content: 'ğŸ¯'; }}
      .icon-risk::before {{ content: 'âš ï¸'; }}
      .icon-innovation::before {{ content: 'ğŸš€'; }}
      
      /* æ‰“å°æ ·å¼ */
      @media print {{
        body {{
          background: white;
          color: black;
          padding: 0;
        }}
        
        .container {{
          box-shadow: none;
          border: none;
          background: white;
        }}
        
        .header {{
          background: #f8f9fa;
          border-bottom: 2px solid #dee2e6;
        }}
        
        .title {{
          color: #2563eb !important;
          -webkit-text-fill-color: #2563eb !important;
        }}
        
        .summary-section,
        .category-section,
        .news-item {{
          background: white;
          border: 1px solid #dee2e6;
          color: black;
        }}
        
        .summary-title,
        .category-title {{
          color: #2563eb;
        }}
        
        .news-title {{
          color: #1f2937;
        }}
        
        .news-analysis,
        .summary-content {{
          color: #374151;
        }}
      }}
    </style>
  
</head>
<body>
  <div class="container">
    <div class="header">
      <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABLAAAAN0CAIAAAArnUa0AAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdd3zV1f3H8fP93pm9SCAhIey9BBkyFJyoOKt1b3FgW2e1WltrW2urXY627j1Q3AMHiooIsqcgG0IGJO/kru/390f6S0NIcs/33pvkwnk9H/3Dws3Nl+Te7z3vcz7nczTTNAUAAAAAQD16d18AAAAAAKB7EAgBAAAAQFEEQgAAAABQFIEQAAAAABRFIAQAAAAARREIAQAAAEBRBEIAAAAAUBSBEAAAAAAURSAEAAAAAEURCAEAAABAUQRCAAAAAFAUgRAAAAAAFEUgBAAAAABFEQgBAAAAQFEEQgAAAABQFIEQAAAAABRFIAQAAAAARREIAQAAAEBRBEIAAAAAUBSBEAAAAAAURSAEAAAAAEURCAEAAABAUQRCAAAAAFAUgRAAAAAAFEUgBAAAAABFEQgBAAAAQFEEQgAAAABQFIEQAAAAABRFIAQAAAAARREIAQAAAEBRBEIAAAAAUBSBEAAAAAAURSAEAAAAAEURCAEAAABAUQRCAAAAAFAUgRAAAAAAFEUgBAAAAABFEQgBAAAAQFEEQgAAAABQFIEQAAAAABRFIAQAAAAARREIAQAAAEBRBEIAAAAAUBSBEAAAAAAURSAEAAAAAEURCAEAAABAUQRCAAAAAFAUgRAAAAAAFEUgBAAAAABFEQgBAAAAQFEEQgAAAABQFIEQAAAAABRFIAQAAAAARREIAQAAAEBRBEIAAAAAUBSBEAAAAAAURSAEAAAAAEURCAEAAABAUQRCAAAAAFAUgRAAAAAAFEUgBAAAAABFEQgBAAAAQFEEQgAAAABQFIEQAAAAABRFIAQAAAAARREIAQAAAEBRBEIAAAAAUBSBEAAAAAAURSAEAAAAAEURCAEAAABAUQRCAAAAAFAUgRAAAAAAFEUgBAAAAABFEQgBAAAAQFEEQgAAAABQFIEQAAAAABRFIAQAAAAARREIAQAAAEBRBEIAAAAAUBSBEAAAAAAURSAEAAAAAEURCAEAAABAUQRCAAAAAFAUgRAAAAAAFEUgBAAAAABFEQgBAAAAQFEEQgAAAABQFIEQAAAAABRFIAQAAAAARREIAQAAAEBRBEIAAAAAUBSBEAAAAAAURSAEAAAAAEURCAEAAABAUQRCAAAAAFAUgRAAAAAAFEUgBAAAAABFEQgBAAAAQFEEQgAAAABQFIEQAAAAABRFIAQAAAAARREIAQAAAEBRBEIAAAAAUBSBEAAAAAAURSAEAAAAAEURCAEAAABAUQRCAAAAAFAUgRAAAAAAFEUgBAAAAABFEQgBAAAAQFEEQgAAAABQFIEQAAAAABRFIAQAAAAARREIAQAAAEBRBEIAAAAA......" alt="Ocean Security Logo" class="logo">
      <h1 class="title">æµ·ä¹‹å®‰å®‰å…¨æ¯æ—¥å¿«æŠ¥</h1>
      <div class="subtitle">{today_str} Â· ç¬¬{news_count}æœŸ</div>
    </div>
    
    <div class="content">
      
        <div class="summary-section">
          <h2 class="summary-title">ä»Šæ—¥æ‘˜è¦</h2>
          <div class="summary-content">{summary_content}</div>
        </div>
      
        <div class="category-section">
          <div class="category-header">
            <h2 class="category-title icon-focus">ä»Šæ—¥å®‰å…¨æ–°é—»</h2>
          </div>
          <div class="category-news">'''

        # æ·»åŠ æ–°é—»æ¡ç›®
        for i, news in enumerate(news_list, 1):
            # ç®€åŒ–æ‘˜è¦å†…å®¹
            summary = self.clean_html_content(news.get('summary', '') or news.get('content', ''))
            if not summary:
                summary = "æš‚æ— è¯¦ç»†æ‘˜è¦å†…å®¹"
            
            html_content += f'''
        <div class="news-item">
          <div class="news-title">{news['title']}</div>
          <div class="news-analysis"><strong>æ¥æºï¼š</strong>{news['source']} | <strong>åˆ†æï¼š</strong>{summary[:300]}{'...' if len(summary) > 300 else ''}</div>
        </div>'''

        html_content += f'''
          </div>
        </div>
      
    </div>
    
    <div class="footer">
      <p>Â© 2025 Ocean Security Â· æµ·ä¹‹å®‰å®‰å…¨æ¯æ—¥å¿«æŠ¥</p>
      <p>Generated on {current_time}</p>
    </div>
  </div>
</body>
</html>'''
        
        return html_content
    
    def save_news_file(self, html_content, filename=None):
        """ä¿å­˜æ–°é—»æ–‡ä»¶"""
        if not html_content:
            logger.error("HTMLå†…å®¹ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜æ–‡ä»¶")
            return False
        
        if filename is None:
            filename = f"news{self.today.strftime('%Y%m%d')}.html"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logger.info(f"æˆåŠŸç”Ÿæˆæ–°é—»æ–‡ä»¶: {filename}")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶ {filename} å¤±è´¥: {e}")
            return False
    
    def run(self):
        """æ‰§è¡Œæ–°é—»æŠ“å–ä¸»æµç¨‹"""
        logger.info("å¼€å§‹æ‰§è¡Œç½‘ç»œå®‰å…¨æ–°é—»è‡ªåŠ¨æŠ“å–...")
        logger.info(f"ç›®æ ‡æ—¥æœŸ: {self.today.strftime('%Y-%m-%d')}")
        
        # æŠ“å–æ‰€æœ‰æºçš„æ–°é—»
        news_list = self.scrape_all_sources()
        
        if not news_list:
            logger.warning("ä»Šæ—¥æœªè·å–åˆ°ä»»ä½•ç½‘ç»œå®‰å…¨æ–°é—»")
            return False
        
        # ç”ŸæˆHTMLå†…å®¹
        html_content = self.generate_html_content(news_list)
        
        if not html_content:
            logger.error("ç”ŸæˆHTMLå†…å®¹å¤±è´¥")
            return False
        
        # ä¿å­˜æ–‡ä»¶
        filename = f"news{self.today.strftime('%Y%m%d')}.html"
        success = self.save_news_file(html_content, filename)
        
        if success:
            logger.info(f"æ–°é—»æŠ“å–å®Œæˆï¼Œå…±ç”Ÿæˆ {len(news_list)} æ¡æ–°é—»")
            logger.info(f"æ–‡ä»¶å·²ä¿å­˜ä¸º: {filename}")
            return True
        else:
            logger.error("æ–‡ä»¶ä¿å­˜å¤±è´¥")
            return False

def main():
    """ä¸»å‡½æ•°"""
    scraper = SecurityNewsScraper()
    success = scraper.run()
    
    if success:
        print("âœ… ç½‘ç»œå®‰å…¨æ–°é—»æŠ“å–æˆåŠŸå®Œæˆï¼")
    else:
        print("âŒ ç½‘ç»œå®‰å…¨æ–°é—»æŠ“å–å¤±è´¥æˆ–æœªè·å–åˆ°æ–°é—»å†…å®¹")

if __name__ == "__main__":
    main()
